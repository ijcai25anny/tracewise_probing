{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9992241044872623,
  "eval_steps": 500,
  "global_step": 1932,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01034527350316824,
      "grad_norm": 0.3048969507217407,
      "learning_rate": 1.9999667766115304e-05,
      "loss": 0.7086,
      "step": 10
    },
    {
      "epoch": 0.02069054700633648,
      "grad_norm": 0.22426724433898926,
      "learning_rate": 1.9997010027491487e-05,
      "loss": 0.6401,
      "step": 20
    },
    {
      "epoch": 0.03103582050950472,
      "grad_norm": 0.22583669424057007,
      "learning_rate": 1.999169525662478e-05,
      "loss": 0.5823,
      "step": 30
    },
    {
      "epoch": 0.04138109401267296,
      "grad_norm": 0.20472505688667297,
      "learning_rate": 1.998372486608929e-05,
      "loss": 0.5393,
      "step": 40
    },
    {
      "epoch": 0.0517263675158412,
      "grad_norm": 0.20995615422725677,
      "learning_rate": 1.9973100974276883e-05,
      "loss": 0.5241,
      "step": 50
    },
    {
      "epoch": 0.06207164101900944,
      "grad_norm": 0.1919674575328827,
      "learning_rate": 1.9959826404834125e-05,
      "loss": 0.5077,
      "step": 60
    },
    {
      "epoch": 0.07241691452217767,
      "grad_norm": 0.22498637437820435,
      "learning_rate": 1.9943904685911826e-05,
      "loss": 0.4994,
      "step": 70
    },
    {
      "epoch": 0.08276218802534592,
      "grad_norm": 0.20224110782146454,
      "learning_rate": 1.9925340049227312e-05,
      "loss": 0.5003,
      "step": 80
    },
    {
      "epoch": 0.09310746152851417,
      "grad_norm": 0.21058668196201324,
      "learning_rate": 1.990413742893971e-05,
      "loss": 0.4833,
      "step": 90
    },
    {
      "epoch": 0.1034527350316824,
      "grad_norm": 0.217948779463768,
      "learning_rate": 1.9880302460338526e-05,
      "loss": 0.4828,
      "step": 100
    },
    {
      "epoch": 0.11379800853485064,
      "grad_norm": 0.23297977447509766,
      "learning_rate": 1.9853841478345894e-05,
      "loss": 0.4803,
      "step": 110
    },
    {
      "epoch": 0.12414328203801887,
      "grad_norm": 0.23659832775592804,
      "learning_rate": 1.9824761515832857e-05,
      "loss": 0.4785,
      "step": 120
    },
    {
      "epoch": 0.13448855554118713,
      "grad_norm": 0.2562071979045868,
      "learning_rate": 1.9793070301750154e-05,
      "loss": 0.4821,
      "step": 130
    },
    {
      "epoch": 0.14483382904435535,
      "grad_norm": 0.21921317279338837,
      "learning_rate": 1.9758776259073994e-05,
      "loss": 0.4563,
      "step": 140
    },
    {
      "epoch": 0.1551791025475236,
      "grad_norm": 0.2575775384902954,
      "learning_rate": 1.9721888502567374e-05,
      "loss": 0.4751,
      "step": 150
    },
    {
      "epoch": 0.16552437605069184,
      "grad_norm": 0.2588975131511688,
      "learning_rate": 1.9682416836357547e-05,
      "loss": 0.4693,
      "step": 160
    },
    {
      "epoch": 0.17586964955386009,
      "grad_norm": 0.2548154294490814,
      "learning_rate": 1.9640371751330242e-05,
      "loss": 0.4633,
      "step": 170
    },
    {
      "epoch": 0.18621492305702833,
      "grad_norm": 0.2760627865791321,
      "learning_rate": 1.9595764422341356e-05,
      "loss": 0.4554,
      "step": 180
    },
    {
      "epoch": 0.19656019656019655,
      "grad_norm": 0.28380122780799866,
      "learning_rate": 1.9548606705246907e-05,
      "loss": 0.4403,
      "step": 190
    },
    {
      "epoch": 0.2069054700633648,
      "grad_norm": 0.2918516993522644,
      "learning_rate": 1.9498911133751917e-05,
      "loss": 0.4526,
      "step": 200
    },
    {
      "epoch": 0.21725074356653304,
      "grad_norm": 0.27196255326271057,
      "learning_rate": 1.944669091607919e-05,
      "loss": 0.4494,
      "step": 210
    },
    {
      "epoch": 0.22759601706970128,
      "grad_norm": 0.2840965688228607,
      "learning_rate": 1.9391959931458782e-05,
      "loss": 0.4535,
      "step": 220
    },
    {
      "epoch": 0.23794129057286953,
      "grad_norm": 0.2945994734764099,
      "learning_rate": 1.9334732726439153e-05,
      "loss": 0.4651,
      "step": 230
    },
    {
      "epoch": 0.24828656407603775,
      "grad_norm": 0.28865882754325867,
      "learning_rate": 1.927502451102095e-05,
      "loss": 0.4608,
      "step": 240
    },
    {
      "epoch": 0.258631837579206,
      "grad_norm": 0.4755766987800598,
      "learning_rate": 1.9212851154614426e-05,
      "loss": 0.444,
      "step": 250
    },
    {
      "epoch": 0.26897711108237427,
      "grad_norm": 0.32103654742240906,
      "learning_rate": 1.9148229181821656e-05,
      "loss": 0.445,
      "step": 260
    },
    {
      "epoch": 0.2793223845855425,
      "grad_norm": 0.38081327080726624,
      "learning_rate": 1.9081175768044575e-05,
      "loss": 0.4684,
      "step": 270
    },
    {
      "epoch": 0.2896676580887107,
      "grad_norm": 0.2681487798690796,
      "learning_rate": 1.901170873492004e-05,
      "loss": 0.4433,
      "step": 280
    },
    {
      "epoch": 0.300012931591879,
      "grad_norm": 0.31533336639404297,
      "learning_rate": 1.8939846545583158e-05,
      "loss": 0.4621,
      "step": 290
    },
    {
      "epoch": 0.3103582050950472,
      "grad_norm": 0.3495166599750519,
      "learning_rate": 1.886560829976011e-05,
      "loss": 0.4338,
      "step": 300
    },
    {
      "epoch": 0.32070347859821546,
      "grad_norm": 0.33331918716430664,
      "learning_rate": 1.878901372869176e-05,
      "loss": 0.4484,
      "step": 310
    },
    {
      "epoch": 0.3310487521013837,
      "grad_norm": 0.3293056786060333,
      "learning_rate": 1.871008318988945e-05,
      "loss": 0.4319,
      "step": 320
    },
    {
      "epoch": 0.3413940256045519,
      "grad_norm": 0.39010730385780334,
      "learning_rate": 1.8628837661724298e-05,
      "loss": 0.4372,
      "step": 330
    },
    {
      "epoch": 0.35173929910772017,
      "grad_norm": 0.33230048418045044,
      "learning_rate": 1.8545298737851554e-05,
      "loss": 0.4277,
      "step": 340
    },
    {
      "epoch": 0.3620845726108884,
      "grad_norm": 0.32965296506881714,
      "learning_rate": 1.845948862147133e-05,
      "loss": 0.4339,
      "step": 350
    },
    {
      "epoch": 0.37242984611405666,
      "grad_norm": 0.3428763747215271,
      "learning_rate": 1.8371430119427414e-05,
      "loss": 0.4417,
      "step": 360
    },
    {
      "epoch": 0.3827751196172249,
      "grad_norm": 0.3750033676624298,
      "learning_rate": 1.8281146636145585e-05,
      "loss": 0.4355,
      "step": 370
    },
    {
      "epoch": 0.3931203931203931,
      "grad_norm": 0.40246057510375977,
      "learning_rate": 1.818866216741312e-05,
      "loss": 0.4325,
      "step": 380
    },
    {
      "epoch": 0.40346566662356137,
      "grad_norm": 0.3319101333618164,
      "learning_rate": 1.8094001294001144e-05,
      "loss": 0.4339,
      "step": 390
    },
    {
      "epoch": 0.4138109401267296,
      "grad_norm": 0.3511982262134552,
      "learning_rate": 1.7997189175131473e-05,
      "loss": 0.4548,
      "step": 400
    },
    {
      "epoch": 0.42415621362989786,
      "grad_norm": 0.3678436875343323,
      "learning_rate": 1.7898251541789738e-05,
      "loss": 0.4306,
      "step": 410
    },
    {
      "epoch": 0.4345014871330661,
      "grad_norm": 0.408006876707077,
      "learning_rate": 1.7797214689886527e-05,
      "loss": 0.4394,
      "step": 420
    },
    {
      "epoch": 0.4448467606362343,
      "grad_norm": 0.36670243740081787,
      "learning_rate": 1.7694105473268384e-05,
      "loss": 0.4526,
      "step": 430
    },
    {
      "epoch": 0.45519203413940257,
      "grad_norm": 0.3743458092212677,
      "learning_rate": 1.7588951296580536e-05,
      "loss": 0.435,
      "step": 440
    },
    {
      "epoch": 0.4655373076425708,
      "grad_norm": 0.3743148744106293,
      "learning_rate": 1.7481780107983197e-05,
      "loss": 0.4358,
      "step": 450
    },
    {
      "epoch": 0.47588258114573906,
      "grad_norm": 0.33927276730537415,
      "learning_rate": 1.7372620391723443e-05,
      "loss": 0.438,
      "step": 460
    },
    {
      "epoch": 0.4862278546489073,
      "grad_norm": 0.38098469376564026,
      "learning_rate": 1.726150116056458e-05,
      "loss": 0.4171,
      "step": 470
    },
    {
      "epoch": 0.4965731281520755,
      "grad_norm": 0.39885127544403076,
      "learning_rate": 1.7148451948075063e-05,
      "loss": 0.4302,
      "step": 480
    },
    {
      "epoch": 0.5069184016552437,
      "grad_norm": 0.39895981550216675,
      "learning_rate": 1.7033502800778986e-05,
      "loss": 0.4274,
      "step": 490
    },
    {
      "epoch": 0.517263675158412,
      "grad_norm": 0.3967888057231903,
      "learning_rate": 1.691668427017022e-05,
      "loss": 0.435,
      "step": 500
    },
    {
      "epoch": 0.5276089486615803,
      "grad_norm": 0.3845967948436737,
      "learning_rate": 1.6798027404592387e-05,
      "loss": 0.4369,
      "step": 510
    },
    {
      "epoch": 0.5379542221647485,
      "grad_norm": 0.41718700528144836,
      "learning_rate": 1.66775637409867e-05,
      "loss": 0.4475,
      "step": 520
    },
    {
      "epoch": 0.5482994956679167,
      "grad_norm": 0.41981738805770874,
      "learning_rate": 1.6555325296510035e-05,
      "loss": 0.4319,
      "step": 530
    },
    {
      "epoch": 0.558644769171085,
      "grad_norm": 0.3789862394332886,
      "learning_rate": 1.6431344560025306e-05,
      "loss": 0.4348,
      "step": 540
    },
    {
      "epoch": 0.5689900426742532,
      "grad_norm": 0.3796037435531616,
      "learning_rate": 1.6305654483466484e-05,
      "loss": 0.4274,
      "step": 550
    },
    {
      "epoch": 0.5793353161774214,
      "grad_norm": 0.3749965727329254,
      "learning_rate": 1.617828847308054e-05,
      "loss": 0.435,
      "step": 560
    },
    {
      "epoch": 0.5896805896805897,
      "grad_norm": 0.36368489265441895,
      "learning_rate": 1.6049280380548665e-05,
      "loss": 0.4256,
      "step": 570
    },
    {
      "epoch": 0.600025863183758,
      "grad_norm": 0.8375405669212341,
      "learning_rate": 1.5918664493989055e-05,
      "loss": 0.4302,
      "step": 580
    },
    {
      "epoch": 0.6103711366869261,
      "grad_norm": 0.42874881625175476,
      "learning_rate": 1.5786475528843737e-05,
      "loss": 0.4242,
      "step": 590
    },
    {
      "epoch": 0.6207164101900944,
      "grad_norm": 0.3942676782608032,
      "learning_rate": 1.5652748618651795e-05,
      "loss": 0.4328,
      "step": 600
    },
    {
      "epoch": 0.6310616836932627,
      "grad_norm": 0.4223795533180237,
      "learning_rate": 1.5517519305711487e-05,
      "loss": 0.4371,
      "step": 610
    },
    {
      "epoch": 0.6414069571964309,
      "grad_norm": 0.3710668087005615,
      "learning_rate": 1.5380823531633727e-05,
      "loss": 0.4155,
      "step": 620
    },
    {
      "epoch": 0.6517522306995991,
      "grad_norm": 0.40161535143852234,
      "learning_rate": 1.5242697627789413e-05,
      "loss": 0.4192,
      "step": 630
    },
    {
      "epoch": 0.6620975042027674,
      "grad_norm": 0.4668026566505432,
      "learning_rate": 1.5103178305653185e-05,
      "loss": 0.4212,
      "step": 640
    },
    {
      "epoch": 0.6724427777059356,
      "grad_norm": 0.42997902631759644,
      "learning_rate": 1.496230264704615e-05,
      "loss": 0.4202,
      "step": 650
    },
    {
      "epoch": 0.6827880512091038,
      "grad_norm": 0.4233785569667816,
      "learning_rate": 1.4820108094280169e-05,
      "loss": 0.4191,
      "step": 660
    },
    {
      "epoch": 0.6931333247122721,
      "grad_norm": 0.41683387756347656,
      "learning_rate": 1.4676632440206342e-05,
      "loss": 0.4113,
      "step": 670
    },
    {
      "epoch": 0.7034785982154403,
      "grad_norm": 0.4297332167625427,
      "learning_rate": 1.4531913818170338e-05,
      "loss": 0.4175,
      "step": 680
    },
    {
      "epoch": 0.7138238717186086,
      "grad_norm": 0.42154985666275024,
      "learning_rate": 1.4385990691877182e-05,
      "loss": 0.4311,
      "step": 690
    },
    {
      "epoch": 0.7241691452217768,
      "grad_norm": 0.4342374801635742,
      "learning_rate": 1.4238901845168294e-05,
      "loss": 0.4236,
      "step": 700
    },
    {
      "epoch": 0.734514418724945,
      "grad_norm": 0.4789721965789795,
      "learning_rate": 1.4090686371713403e-05,
      "loss": 0.4238,
      "step": 710
    },
    {
      "epoch": 0.7448596922281133,
      "grad_norm": 0.3852367401123047,
      "learning_rate": 1.3941383664620118e-05,
      "loss": 0.4285,
      "step": 720
    },
    {
      "epoch": 0.7552049657312815,
      "grad_norm": 0.4548547863960266,
      "learning_rate": 1.379103340596395e-05,
      "loss": 0.4202,
      "step": 730
    },
    {
      "epoch": 0.7655502392344498,
      "grad_norm": 0.43976089358329773,
      "learning_rate": 1.363967555624147e-05,
      "loss": 0.4293,
      "step": 740
    },
    {
      "epoch": 0.775895512737618,
      "grad_norm": 0.39953354001045227,
      "learning_rate": 1.3487350343749521e-05,
      "loss": 0.4231,
      "step": 750
    },
    {
      "epoch": 0.7862407862407862,
      "grad_norm": 0.4493659734725952,
      "learning_rate": 1.3334098253893231e-05,
      "loss": 0.4091,
      "step": 760
    },
    {
      "epoch": 0.7965860597439545,
      "grad_norm": 0.43220555782318115,
      "learning_rate": 1.3179960018425682e-05,
      "loss": 0.4323,
      "step": 770
    },
    {
      "epoch": 0.8069313332471227,
      "grad_norm": 0.4691464304924011,
      "learning_rate": 1.3024976604622125e-05,
      "loss": 0.4416,
      "step": 780
    },
    {
      "epoch": 0.817276606750291,
      "grad_norm": 0.4252246618270874,
      "learning_rate": 1.2869189204391595e-05,
      "loss": 0.4321,
      "step": 790
    },
    {
      "epoch": 0.8276218802534592,
      "grad_norm": 0.4522128403186798,
      "learning_rate": 1.2712639223328789e-05,
      "loss": 0.418,
      "step": 800
    },
    {
      "epoch": 0.8379671537566274,
      "grad_norm": 0.44113749265670776,
      "learning_rate": 1.2555368269709198e-05,
      "loss": 0.4281,
      "step": 810
    },
    {
      "epoch": 0.8483124272597957,
      "grad_norm": 0.42015719413757324,
      "learning_rate": 1.2397418143430344e-05,
      "loss": 0.4135,
      "step": 820
    },
    {
      "epoch": 0.8586577007629639,
      "grad_norm": 0.4288116693496704,
      "learning_rate": 1.2238830824902078e-05,
      "loss": 0.4178,
      "step": 830
    },
    {
      "epoch": 0.8690029742661322,
      "grad_norm": 0.4632304012775421,
      "learning_rate": 1.2079648463888932e-05,
      "loss": 0.4261,
      "step": 840
    },
    {
      "epoch": 0.8793482477693004,
      "grad_norm": 0.43681418895721436,
      "learning_rate": 1.1919913368307425e-05,
      "loss": 0.4155,
      "step": 850
    },
    {
      "epoch": 0.8896935212724686,
      "grad_norm": 0.4373975396156311,
      "learning_rate": 1.1759667992981362e-05,
      "loss": 0.414,
      "step": 860
    },
    {
      "epoch": 0.9000387947756369,
      "grad_norm": 0.4598906338214874,
      "learning_rate": 1.1598954928358074e-05,
      "loss": 0.4101,
      "step": 870
    },
    {
      "epoch": 0.9103840682788051,
      "grad_norm": 0.48395881056785583,
      "learning_rate": 1.1437816889188606e-05,
      "loss": 0.4038,
      "step": 880
    },
    {
      "epoch": 0.9207293417819734,
      "grad_norm": 0.483720988035202,
      "learning_rate": 1.1276296703174887e-05,
      "loss": 0.4179,
      "step": 890
    },
    {
      "epoch": 0.9310746152851416,
      "grad_norm": 0.4589798152446747,
      "learning_rate": 1.1114437299586864e-05,
      "loss": 0.4168,
      "step": 900
    },
    {
      "epoch": 0.9414198887883098,
      "grad_norm": 0.42642900347709656,
      "learning_rate": 1.0952281697852642e-05,
      "loss": 0.4186,
      "step": 910
    },
    {
      "epoch": 0.9517651622914781,
      "grad_norm": 0.4054805636405945,
      "learning_rate": 1.0789872996124664e-05,
      "loss": 0.4091,
      "step": 920
    },
    {
      "epoch": 0.9621104357946463,
      "grad_norm": 0.43209853768348694,
      "learning_rate": 1.0627254359824981e-05,
      "loss": 0.4264,
      "step": 930
    },
    {
      "epoch": 0.9724557092978146,
      "grad_norm": 0.48021501302719116,
      "learning_rate": 1.0464469010172605e-05,
      "loss": 0.4136,
      "step": 940
    },
    {
      "epoch": 0.9828009828009828,
      "grad_norm": 0.4586613178253174,
      "learning_rate": 1.0301560212696087e-05,
      "loss": 0.4217,
      "step": 950
    },
    {
      "epoch": 0.993146256304151,
      "grad_norm": 0.503411054611206,
      "learning_rate": 1.013857126573426e-05,
      "loss": 0.4139,
      "step": 960
    },
    {
      "epoch": 1.0040087934824777,
      "grad_norm": 0.5076871514320374,
      "learning_rate": 9.975545488928315e-06,
      "loss": 0.4542,
      "step": 970
    },
    {
      "epoch": 1.014354066985646,
      "grad_norm": 0.4770282506942749,
      "learning_rate": 9.812526211708164e-06,
      "loss": 0.398,
      "step": 980
    },
    {
      "epoch": 1.0246993404888143,
      "grad_norm": 0.5066853165626526,
      "learning_rate": 9.649556761776258e-06,
      "loss": 0.4034,
      "step": 990
    },
    {
      "epoch": 1.0350446139919824,
      "grad_norm": 0.48679402470588684,
      "learning_rate": 9.4866804535918e-06,
      "loss": 0.42,
      "step": 1000
    },
    {
      "epoch": 1.0453898874951506,
      "grad_norm": 0.43463489413261414,
      "learning_rate": 9.32394057685855e-06,
      "loss": 0.4069,
      "step": 1010
    },
    {
      "epoch": 1.055735160998319,
      "grad_norm": 0.46669742465019226,
      "learning_rate": 9.16138038501912e-06,
      "loss": 0.4048,
      "step": 1020
    },
    {
      "epoch": 1.0660804345014872,
      "grad_norm": 0.5325942635536194,
      "learning_rate": 8.999043083759016e-06,
      "loss": 0.4175,
      "step": 1030
    },
    {
      "epoch": 1.0764257080046553,
      "grad_norm": 0.5657998919487,
      "learning_rate": 8.836971819523273e-06,
      "loss": 0.3998,
      "step": 1040
    },
    {
      "epoch": 1.0867709815078237,
      "grad_norm": 0.4785939157009125,
      "learning_rate": 8.675209668048886e-06,
      "loss": 0.4052,
      "step": 1050
    },
    {
      "epoch": 1.0971162550109919,
      "grad_norm": 0.47531798481941223,
      "learning_rate": 8.513799622916034e-06,
      "loss": 0.4176,
      "step": 1060
    },
    {
      "epoch": 1.10746152851416,
      "grad_norm": 0.4826699197292328,
      "learning_rate": 8.35278458412112e-06,
      "loss": 0.4125,
      "step": 1070
    },
    {
      "epoch": 1.1178068020173284,
      "grad_norm": 0.5128738284111023,
      "learning_rate": 8.192207346674707e-06,
      "loss": 0.4072,
      "step": 1080
    },
    {
      "epoch": 1.1281520755204966,
      "grad_norm": 0.5131810903549194,
      "learning_rate": 8.032110589227324e-06,
      "loss": 0.4004,
      "step": 1090
    },
    {
      "epoch": 1.1384973490236647,
      "grad_norm": 0.5075618028640747,
      "learning_rate": 7.872536862726245e-06,
      "loss": 0.4156,
      "step": 1100
    },
    {
      "epoch": 1.1488426225268331,
      "grad_norm": 0.7523699402809143,
      "learning_rate": 7.713528579106158e-06,
      "loss": 0.3931,
      "step": 1110
    },
    {
      "epoch": 1.1591878960300013,
      "grad_norm": 0.5057762265205383,
      "learning_rate": 7.555128000016836e-06,
      "loss": 0.3948,
      "step": 1120
    },
    {
      "epoch": 1.1695331695331694,
      "grad_norm": 0.4569109380245209,
      "learning_rate": 7.397377225590687e-06,
      "loss": 0.4008,
      "step": 1130
    },
    {
      "epoch": 1.1798784430363378,
      "grad_norm": 0.4581877589225769,
      "learning_rate": 7.240318183253304e-06,
      "loss": 0.4103,
      "step": 1140
    },
    {
      "epoch": 1.190223716539506,
      "grad_norm": 0.4560423791408539,
      "learning_rate": 7.083992616579846e-06,
      "loss": 0.4092,
      "step": 1150
    },
    {
      "epoch": 1.2005689900426741,
      "grad_norm": 0.5412558317184448,
      "learning_rate": 6.92844207420035e-06,
      "loss": 0.414,
      "step": 1160
    },
    {
      "epoch": 1.2109142635458425,
      "grad_norm": 0.5010860562324524,
      "learning_rate": 6.773707898756797e-06,
      "loss": 0.4075,
      "step": 1170
    },
    {
      "epoch": 1.2212595370490107,
      "grad_norm": 0.47721174359321594,
      "learning_rate": 6.619831215914974e-06,
      "loss": 0.3914,
      "step": 1180
    },
    {
      "epoch": 1.231604810552179,
      "grad_norm": 0.5177661180496216,
      "learning_rate": 6.466852923433968e-06,
      "loss": 0.4152,
      "step": 1190
    },
    {
      "epoch": 1.2419500840553472,
      "grad_norm": 0.4892740249633789,
      "learning_rate": 6.31481368029627e-06,
      "loss": 0.4128,
      "step": 1200
    },
    {
      "epoch": 1.2522953575585154,
      "grad_norm": 0.5151373744010925,
      "learning_rate": 6.1637538959012925e-06,
      "loss": 0.4043,
      "step": 1210
    },
    {
      "epoch": 1.2626406310616836,
      "grad_norm": 0.5357569456100464,
      "learning_rate": 6.013713719325276e-06,
      "loss": 0.4076,
      "step": 1220
    },
    {
      "epoch": 1.272985904564852,
      "grad_norm": 0.5066627264022827,
      "learning_rate": 5.864733028650344e-06,
      "loss": 0.4068,
      "step": 1230
    },
    {
      "epoch": 1.28333117806802,
      "grad_norm": 0.4960200786590576,
      "learning_rate": 5.71685142036561e-06,
      "loss": 0.4066,
      "step": 1240
    },
    {
      "epoch": 1.2936764515711885,
      "grad_norm": 0.46404704451560974,
      "learning_rate": 5.570108198843108e-06,
      "loss": 0.4005,
      "step": 1250
    },
    {
      "epoch": 1.3040217250743567,
      "grad_norm": 0.4834386110305786,
      "learning_rate": 5.424542365891363e-06,
      "loss": 0.4112,
      "step": 1260
    },
    {
      "epoch": 1.3143669985775248,
      "grad_norm": 0.5069231390953064,
      "learning_rate": 5.280192610389422e-06,
      "loss": 0.4094,
      "step": 1270
    },
    {
      "epoch": 1.3247122720806932,
      "grad_norm": 0.4659605324268341,
      "learning_rate": 5.137097298003962e-06,
      "loss": 0.4143,
      "step": 1280
    },
    {
      "epoch": 1.3350575455838614,
      "grad_norm": 0.5072289109230042,
      "learning_rate": 4.99529446099241e-06,
      "loss": 0.4015,
      "step": 1290
    },
    {
      "epoch": 1.3454028190870297,
      "grad_norm": 0.5296571254730225,
      "learning_rate": 4.854821788094602e-06,
      "loss": 0.3984,
      "step": 1300
    },
    {
      "epoch": 1.355748092590198,
      "grad_norm": 0.48315951228141785,
      "learning_rate": 4.71571661451578e-06,
      "loss": 0.3969,
      "step": 1310
    },
    {
      "epoch": 1.366093366093366,
      "grad_norm": 0.4820355474948883,
      "learning_rate": 4.57801591200352e-06,
      "loss": 0.401,
      "step": 1320
    },
    {
      "epoch": 1.3764386395965342,
      "grad_norm": 0.5209475159645081,
      "learning_rate": 4.441756279021319e-06,
      "loss": 0.4052,
      "step": 1330
    },
    {
      "epoch": 1.3867839130997026,
      "grad_norm": 0.5054828524589539,
      "learning_rate": 4.30697393102131e-06,
      "loss": 0.4033,
      "step": 1340
    },
    {
      "epoch": 1.3971291866028708,
      "grad_norm": 0.5252093076705933,
      "learning_rate": 4.173704690818862e-06,
      "loss": 0.4151,
      "step": 1350
    },
    {
      "epoch": 1.4074744601060392,
      "grad_norm": 0.4965786635875702,
      "learning_rate": 4.041983979071474e-06,
      "loss": 0.4007,
      "step": 1360
    },
    {
      "epoch": 1.4178197336092073,
      "grad_norm": 0.5259242653846741,
      "learning_rate": 3.911846804864589e-06,
      "loss": 0.4107,
      "step": 1370
    },
    {
      "epoch": 1.4281650071123755,
      "grad_norm": 0.5536547303199768,
      "learning_rate": 3.7833277564067627e-06,
      "loss": 0.4025,
      "step": 1380
    },
    {
      "epoch": 1.4385102806155436,
      "grad_norm": 0.5429705381393433,
      "learning_rate": 3.6564609918367467e-06,
      "loss": 0.3973,
      "step": 1390
    },
    {
      "epoch": 1.448855554118712,
      "grad_norm": 0.4806370735168457,
      "learning_rate": 3.5312802301448334e-06,
      "loss": 0.3945,
      "step": 1400
    },
    {
      "epoch": 1.4592008276218802,
      "grad_norm": 0.4884862005710602,
      "learning_rate": 3.4078187422109157e-06,
      "loss": 0.3901,
      "step": 1410
    },
    {
      "epoch": 1.4695461011250486,
      "grad_norm": 0.4681548774242401,
      "learning_rate": 3.286109341961691e-06,
      "loss": 0.4043,
      "step": 1420
    },
    {
      "epoch": 1.4798913746282167,
      "grad_norm": 0.5717136859893799,
      "learning_rate": 3.1661843776492705e-06,
      "loss": 0.3878,
      "step": 1430
    },
    {
      "epoch": 1.490236648131385,
      "grad_norm": 0.5283384323120117,
      "learning_rate": 3.0480757232535773e-06,
      "loss": 0.3932,
      "step": 1440
    },
    {
      "epoch": 1.500581921634553,
      "grad_norm": 0.5681130886077881,
      "learning_rate": 2.9318147700107946e-06,
      "loss": 0.4064,
      "step": 1450
    },
    {
      "epoch": 1.5109271951377214,
      "grad_norm": 0.5306481122970581,
      "learning_rate": 2.8174324180701195e-06,
      "loss": 0.3996,
      "step": 1460
    },
    {
      "epoch": 1.5212724686408898,
      "grad_norm": 0.4831627309322357,
      "learning_rate": 2.704959068281029e-06,
      "loss": 0.4162,
      "step": 1470
    },
    {
      "epoch": 1.531617742144058,
      "grad_norm": 0.5061665773391724,
      "learning_rate": 2.594424614113277e-06,
      "loss": 0.4076,
      "step": 1480
    },
    {
      "epoch": 1.5419630156472262,
      "grad_norm": 0.5397015810012817,
      "learning_rate": 2.4858584337117166e-06,
      "loss": 0.4083,
      "step": 1490
    },
    {
      "epoch": 1.5523082891503943,
      "grad_norm": 0.5554867386817932,
      "learning_rate": 2.379289382088099e-06,
      "loss": 0.41,
      "step": 1500
    },
    {
      "epoch": 1.5626535626535627,
      "grad_norm": 0.5210947394371033,
      "learning_rate": 2.2747457834519037e-06,
      "loss": 0.3865,
      "step": 1510
    },
    {
      "epoch": 1.5729988361567309,
      "grad_norm": 0.537807047367096,
      "learning_rate": 2.17225542368228e-06,
      "loss": 0.4035,
      "step": 1520
    },
    {
      "epoch": 1.5833441096598992,
      "grad_norm": 0.5592686533927917,
      "learning_rate": 2.071845542943003e-06,
      "loss": 0.4021,
      "step": 1530
    },
    {
      "epoch": 1.5936893831630674,
      "grad_norm": 0.5739713907241821,
      "learning_rate": 1.9735428284425393e-06,
      "loss": 0.4003,
      "step": 1540
    },
    {
      "epoch": 1.6040346566662356,
      "grad_norm": 0.49106961488723755,
      "learning_rate": 1.877373407341041e-06,
      "loss": 0.4012,
      "step": 1550
    },
    {
      "epoch": 1.6143799301694037,
      "grad_norm": 0.5253879427909851,
      "learning_rate": 1.7833628398061875e-06,
      "loss": 0.4042,
      "step": 1560
    },
    {
      "epoch": 1.6247252036725721,
      "grad_norm": 0.5232608914375305,
      "learning_rate": 1.6915361122197504e-06,
      "loss": 0.3898,
      "step": 1570
    },
    {
      "epoch": 1.6350704771757405,
      "grad_norm": 0.5015397071838379,
      "learning_rate": 1.6019176305366401e-06,
      "loss": 0.3978,
      "step": 1580
    },
    {
      "epoch": 1.6454157506789087,
      "grad_norm": 0.470867395401001,
      "learning_rate": 1.514531213798225e-06,
      "loss": 0.4055,
      "step": 1590
    },
    {
      "epoch": 1.6557610241820768,
      "grad_norm": 0.5254515409469604,
      "learning_rate": 1.4294000878016356e-06,
      "loss": 0.3992,
      "step": 1600
    },
    {
      "epoch": 1.666106297685245,
      "grad_norm": 0.5470215082168579,
      "learning_rate": 1.34654687892677e-06,
      "loss": 0.3967,
      "step": 1610
    },
    {
      "epoch": 1.6764515711884131,
      "grad_norm": 0.5187625885009766,
      "learning_rate": 1.2659936081225654e-06,
      "loss": 0.4064,
      "step": 1620
    },
    {
      "epoch": 1.6867968446915815,
      "grad_norm": 0.533759593963623,
      "learning_rate": 1.187761685054234e-06,
      "loss": 0.3903,
      "step": 1630
    },
    {
      "epoch": 1.69714211819475,
      "grad_norm": 0.5107550621032715,
      "learning_rate": 1.1118719024129377e-06,
      "loss": 0.4041,
      "step": 1640
    },
    {
      "epoch": 1.707487391697918,
      "grad_norm": 0.5659550428390503,
      "learning_rate": 1.0383444303894453e-06,
      "loss": 0.4111,
      "step": 1650
    },
    {
      "epoch": 1.7178326652010862,
      "grad_norm": 0.5360966920852661,
      "learning_rate": 9.67198811313248e-07,
      "loss": 0.4121,
      "step": 1660
    },
    {
      "epoch": 1.7281779387042544,
      "grad_norm": 0.5538782477378845,
      "learning_rate": 8.984539544585502e-07,
      "loss": 0.4041,
      "step": 1670
    },
    {
      "epoch": 1.7385232122074228,
      "grad_norm": 0.5432442426681519,
      "learning_rate": 8.321281310185081e-07,
      "loss": 0.4084,
      "step": 1680
    },
    {
      "epoch": 1.748868485710591,
      "grad_norm": 0.5081606507301331,
      "learning_rate": 7.682389692490666e-07,
      "loss": 0.4059,
      "step": 1690
    },
    {
      "epoch": 1.7592137592137593,
      "grad_norm": 0.5017808675765991,
      "learning_rate": 7.068034497836607e-07,
      "loss": 0.4101,
      "step": 1700
    },
    {
      "epoch": 1.7695590327169275,
      "grad_norm": 0.5272195935249329,
      "learning_rate": 6.47837901120083e-07,
      "loss": 0.4012,
      "step": 1710
    },
    {
      "epoch": 1.7799043062200957,
      "grad_norm": 0.5287110209465027,
      "learning_rate": 5.913579952806192e-07,
      "loss": 0.4008,
      "step": 1720
    },
    {
      "epoch": 1.7902495797232638,
      "grad_norm": 0.5422398447990417,
      "learning_rate": 5.373787436467248e-07,
      "loss": 0.4148,
      "step": 1730
    },
    {
      "epoch": 1.8005948532264322,
      "grad_norm": 0.590404212474823,
      "learning_rate": 4.859144929692416e-07,
      "loss": 0.4097,
      "step": 1740
    },
    {
      "epoch": 1.8109401267296004,
      "grad_norm": 0.5096516609191895,
      "learning_rate": 4.3697892155528446e-07,
      "loss": 0.4043,
      "step": 1750
    },
    {
      "epoch": 1.8212854002327687,
      "grad_norm": 0.5185092091560364,
      "learning_rate": 3.9058503563279004e-07,
      "loss": 0.4005,
      "step": 1760
    },
    {
      "epoch": 1.831630673735937,
      "grad_norm": 0.4892966151237488,
      "learning_rate": 3.467451658936749e-07,
      "loss": 0.3873,
      "step": 1770
    },
    {
      "epoch": 1.841975947239105,
      "grad_norm": 0.5243841409683228,
      "learning_rate": 3.054709642165654e-07,
      "loss": 0.4096,
      "step": 1780
    },
    {
      "epoch": 1.8523212207422732,
      "grad_norm": 0.4868890345096588,
      "learning_rate": 2.667734005699119e-07,
      "loss": 0.408,
      "step": 1790
    },
    {
      "epoch": 1.8626664942454416,
      "grad_norm": 0.47720637917518616,
      "learning_rate": 2.3066276009638066e-07,
      "loss": 0.4193,
      "step": 1800
    },
    {
      "epoch": 1.87301176774861,
      "grad_norm": 0.5282235741615295,
      "learning_rate": 1.9714864037922666e-07,
      "loss": 0.4136,
      "step": 1810
    },
    {
      "epoch": 1.8833570412517782,
      "grad_norm": 0.5443780422210693,
      "learning_rate": 1.662399488914279e-07,
      "loss": 0.3974,
      "step": 1820
    },
    {
      "epoch": 1.8937023147549463,
      "grad_norm": 0.49998319149017334,
      "learning_rate": 1.3794490062823297e-07,
      "loss": 0.4096,
      "step": 1830
    },
    {
      "epoch": 1.9040475882581145,
      "grad_norm": 0.5315237045288086,
      "learning_rate": 1.12271015923755e-07,
      "loss": 0.4037,
      "step": 1840
    },
    {
      "epoch": 1.9143928617612826,
      "grad_norm": 0.5728899836540222,
      "learning_rate": 8.922511845219972e-08,
      "loss": 0.4155,
      "step": 1850
    },
    {
      "epoch": 1.924738135264451,
      "grad_norm": 0.5313774943351746,
      "learning_rate": 6.881333341425045e-08,
      "loss": 0.3953,
      "step": 1860
    },
    {
      "epoch": 1.9350834087676194,
      "grad_norm": 0.46478715538978577,
      "learning_rate": 5.1041085909094844e-08,
      "loss": 0.3888,
      "step": 1870
    },
    {
      "epoch": 1.9454286822707876,
      "grad_norm": 0.5340573787689209,
      "learning_rate": 3.5913099492528305e-08,
      "loss": 0.4075,
      "step": 1880
    },
    {
      "epoch": 1.9557739557739557,
      "grad_norm": 0.5252890586853027,
      "learning_rate": 2.3433394921514862e-08,
      "loss": 0.3951,
      "step": 1890
    },
    {
      "epoch": 1.966119229277124,
      "grad_norm": 0.5607122778892517,
      "learning_rate": 1.3605289085536488e-08,
      "loss": 0.411,
      "step": 1900
    },
    {
      "epoch": 1.9764645027802923,
      "grad_norm": 0.5025768280029297,
      "learning_rate": 6.431394125024915e-09,
      "loss": 0.4026,
      "step": 1910
    },
    {
      "epoch": 1.9868097762834604,
      "grad_norm": 0.5182703733444214,
      "learning_rate": 1.9136167371014423e-09,
      "loss": 0.4047,
      "step": 1920
    },
    {
      "epoch": 1.9971550497866288,
      "grad_norm": 0.48887768387794495,
      "learning_rate": 5.315766880342743e-11,
      "loss": 0.4167,
      "step": 1930
    },
    {
      "epoch": 1.9992241044872623,
      "step": 1932,
      "total_flos": 7.019475574506652e+18,
      "train_loss": 0.4254303328986,
      "train_runtime": 14917.3506,
      "train_samples_per_second": 12.441,
      "train_steps_per_second": 0.13
    }
  ],
  "logging_steps": 10,
  "max_steps": 1932,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.019475574506652e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
